# -*- coding: utf-8 -*-
"""StudyMate.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JMprb55cIsB51Ah0ymLjhEslAjh4D-4T
"""

# StudyMate - AI-Powered Study Assistant
# Run this entire cell in Google Colab

# Install required packages
!pip install -q gradio transformers torch PyPDF2 pytesseract Pillow sentence-transformers faiss-cpu gtts requests SpeechRecognition pydub
!apt-get install -y tesseract-ocr portaudio19-dev python3-pyaudio

import gradio as gr
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from sentence_transformers import SentenceTransformer
import PyPDF2
import io
import os
import json
from datetime import datetime
import faiss
import numpy as np
from PIL import Image
import pytesseract
import random
from gtts import gTTS
import base64
import requests
import urllib.parse
import speech_recognition as sr
import re

# Global variables
pdf_texts = []
pdf_names = []
qa_history = []
embeddings_db = None
embedding_model = None
llm_model = None
tokenizer = None
flashcards = []
all_chunks = []
current_flashcard_index = 0
quiz_questions = []
user_answers = {}

# Initialize models
def initialize_models():
    global embedding_model, llm_model, tokenizer

    print("Loading IBM Granite 3.3 2B model...")
    model_name = "ibm-granite/granite-3.0-2b-instruct"

    tokenizer = AutoTokenizer.from_pretrained(model_name)
    llm_model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
        device_map="auto" if torch.cuda.is_available() else None
    )

    print("Loading embedding model...")
    embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

    print("Models loaded successfully!")

# PDF Processing
def extract_text_from_pdf(pdf_file):
    try:
        pdf_reader = PyPDF2.PdfReader(io.BytesIO(pdf_file))
        text = ""
        for page_num, page in enumerate(pdf_reader.pages):
            try:
                page_text = page.extract_text()
                if page_text:
                    text += page_text + "\n"
            except Exception as e:
                print(f"Error extracting page {page_num}: {str(e)}")
                continue

        if not text.strip():
            return "Error: No text could be extracted from PDF"
        return text
    except Exception as e:
        return f"Error extracting PDF: {str(e)}"

def process_pdfs(files):
    global pdf_texts, pdf_names, embeddings_db, all_chunks

    if not files:
        return "‚ö†Ô∏è No files uploaded", "", get_stats()

    pdf_texts = []
    pdf_names = []
    all_chunks = []

    for file in files:
        text = extract_text_from_pdf(file)
        if not text.startswith("Error"):
            pdf_texts.append(text)
            pdf_names.append(file.name)

            chunks = [text[i:i+500] for i in range(0, len(text), 500) if text[i:i+500].strip()]
            all_chunks.extend(chunks)

    if all_chunks:
        try:
            embeddings = embedding_model.encode(all_chunks)
            dimension = embeddings.shape[1]
            embeddings_db = faiss.IndexFlatL2(dimension)
            embeddings_db.add(np.array(embeddings).astype('float32'))
        except Exception as e:
            return f"‚ùå Error creating embeddings: {str(e)}", "", get_stats()

    pdf_list_text = "\n".join([f"‚úì {name}" for name in pdf_names])
    return f"‚úÖ Successfully processed {len(pdf_names)} document(s)!", pdf_list_text, get_stats()

# OCR Processing
def process_image_ocr(image):
    global pdf_texts, pdf_names, embeddings_db, all_chunks

    if image is None:
        return "‚ö†Ô∏è No image uploaded", get_stats()

    try:
        image = image.convert('L')
        custom_config = r'--oem 3 --psm 6'
        text = pytesseract.image_to_string(image, config=custom_config)

        if not text.strip():
            return "‚ö†Ô∏è No text could be extracted from the image", get_stats()

        pdf_texts.append(text)
        pdf_names.append(f"OCR_Image_{len(pdf_texts)}")

        chunks = [text[i:i+500] for i in range(0, len(text), 500) if text[i:i+500].strip()]
        all_chunks.extend(chunks)

        if all_chunks:
            embeddings = embedding_model.encode(all_chunks)
            dimension = embeddings.shape[1]
            embeddings_db = faiss.IndexFlatL2(dimension)
            embeddings_db.add(np.array(embeddings).astype('float32'))

        return f"‚úÖ Text extracted successfully!\n\n{text[:500]}...", get_stats()
    except Exception as e:
        return f"‚ùå Error processing image: {str(e)}", get_stats()

# Retrieval function
def retrieve_relevant_chunks(query, top_k=3):
    global embeddings_db, all_chunks

    if embeddings_db is None or not all_chunks:
        return []

    try:
        query_embedding = embedding_model.encode([query])
        distances, indices = embeddings_db.search(np.array(query_embedding).astype('float32'), top_k)
        relevant_chunks = [all_chunks[idx] for idx in indices[0] if idx < len(all_chunks)]
        return relevant_chunks
    except Exception as e:
        print(f"Error retrieving chunks: {str(e)}")
        return []

# Q&A function
def answer_question(question, history):
    global qa_history

    if not pdf_texts:
        return history + [(question, "‚ö†Ô∏è Please upload documents first in the Data Sources tab.")], "", "", get_stats()

    if not question.strip():
        return history, "", "", get_stats()

    relevant_chunks = retrieve_relevant_chunks(question, top_k=3)
    context = "\n\n".join(relevant_chunks)

    prompt = f"""Based on the following context, answer the question concisely and accurately.

Context:
{context}

Question: {question}

Answer:"""

    try:
        inputs = tokenizer(prompt, return_tensors="pt", max_length=1024, truncation=True)
        if torch.cuda.is_available():
            inputs = {k: v.to('cuda') for k, v in inputs.items()}

        outputs = llm_model.generate(
            **inputs,
            max_new_tokens=256,
            temperature=0.7,
            do_sample=True,
            top_p=0.9
        )

        answer = tokenizer.decode(outputs[0], skip_special_tokens=True)
        answer = answer.split("Answer:")[-1].strip()

        qa_entry = {
            "question": question,
            "answer": answer,
            "context": relevant_chunks,
            "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        }
        qa_history.append(qa_entry)

        sources = "\n\n‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n\n".join([
            f"üìÑ Source {i+1}:\n{chunk[:250]}..."
            for i, chunk in enumerate(relevant_chunks)
        ])

        history = history + [(question, answer)]
        return history, "", sources, get_stats()
    except Exception as e:
        error_msg = f"‚ùå Error generating answer: {str(e)}"
        return history + [(question, error_msg)], "", "", get_stats()

# Voice Assistant - Fixed with TTS output
def process_voice_input(audio_file, voice_history):
    if audio_file is None:
        return "‚ö†Ô∏è No audio recorded", voice_history, None

    try:
        recognizer = sr.Recognizer()

        # Load audio file
        with sr.AudioFile(audio_file) as source:
            recognizer.adjust_for_ambient_noise(source, duration=0.5)
            audio_data = recognizer.record(source)

        # Transcribe
        text = recognizer.recognize_google(audio_data)

        if not pdf_texts:
            return f"üé§ Transcribed: \"{text}\"", voice_history + [(text, "‚ö†Ô∏è Please upload documents first in the Data Sources tab.")], None

        # Get answer
        relevant_chunks = retrieve_relevant_chunks(text, top_k=3)
        context = "\n\n".join(relevant_chunks)

        prompt = f"""Based on the following context, answer the question concisely and accurately.

Context:
{context}

Question: {text}

Answer:"""

        inputs = tokenizer(prompt, return_tensors="pt", max_length=1024, truncation=True)
        if torch.cuda.is_available():
            inputs = {k: v.to('cuda') for k, v in inputs.items()}

        outputs = llm_model.generate(
            **inputs,
            max_new_tokens=256,
            temperature=0.7,
            do_sample=True,
            top_p=0.9
        )

        answer = tokenizer.decode(outputs[0], skip_special_tokens=True)
        answer = answer.split("Answer:")[-1].strip()

        # Generate audio response
        try:
            tts = gTTS(text=answer, lang='en', slow=False)
            audio_filename = f"voice_response_{datetime.now().strftime('%Y%m%d_%H%M%S')}.mp3"
            tts.save(audio_filename)
        except Exception as e:
            print(f"TTS Error: {e}")
            audio_filename = None

        voice_history = voice_history + [(text, answer)]

        return f"üé§ You asked: \"{text}\"", voice_history, audio_filename

    except sr.UnknownValueError:
        return "‚ùå Could not understand audio. Please speak clearly.", voice_history, None
    except sr.RequestError as e:
        return f"‚ùå Speech recognition error: {str(e)}", voice_history, None
    except Exception as e:
        return f"‚ùå Error: {str(e)}", voice_history, None

# Interactive Quiz Generator - FIXED for 1-mark questions
def generate_quiz(topic, num_questions):
    global quiz_questions, user_answers

    if not pdf_texts:
        return "‚ö†Ô∏è Please upload documents first in the Data Sources tab."

    if not topic or not topic.strip():
        return "‚ö†Ô∏è Please enter a topic for the quiz."

    try:
        num_questions = int(num_questions)
        if num_questions < 1 or num_questions > 10:
            return "‚ö†Ô∏è Please enter a number between 1 and 10"
    except:
        return "‚ö†Ô∏è Please enter a valid number"

    try:
        # Get topic-relevant content
        relevant_chunks = retrieve_relevant_chunks(topic, top_k=8)
        text_sample = "\n\n".join(relevant_chunks)

        if not text_sample.strip():
            return f"‚ö†Ô∏è No content found related to '{topic}'. Try a different topic."

        prompt = f"""Create {num_questions} simple multiple-choice questions about {topic} based ONLY on this content.

Content:
{text_sample}

Requirements:
- Questions should be simple and direct (1 mark each)
- Options should be SHORT (1-3 words maximum)
- Use only information from the content above
- Format: Q1|question text|short option A|short option B|short option C|short option D|correct letter

Generate {num_questions} questions:"""

        inputs = tokenizer(prompt, return_tensors="pt", max_length=1200, truncation=True)
        if torch.cuda.is_available():
            inputs = {k: v.to('cuda') for k, v in inputs.items()}

        outputs = llm_model.generate(
            **inputs,
            max_new_tokens=1200,
            temperature=0.75,
            do_sample=True,
            top_p=0.9
        )

        result = tokenizer.decode(outputs[0], skip_special_tokens=True)

        quiz_questions = []
        user_answers = {}

        # Parse pipe-separated format
        lines = result.split('\n')
        for line in lines:
            if '|' in line and len(line.split('|')) >= 7:
                parts = line.split('|')
                try:
                    question_text = parts[1].strip()

                    # Skip placeholder/example questions
                    skip_phrases = ['what is the question', 'question text', 'question from', 'example', 'photosynthesis', 'powerhouse of cell']
                    if any(phrase in question_text.lower() for phrase in skip_phrases):
                        continue

                    # Ensure options are short
                    options = [parts[2].strip(), parts[3].strip(), parts[4].strip(), parts[5].strip()]

                    # Skip if any option is too long (more than 50 chars indicates not following format)
                    if any(len(opt) > 50 for opt in options):
                        continue

                    correct = parts[6].strip()[0].upper()
                    if correct not in ['A', 'B', 'C', 'D']:
                        correct = 'A'

                    quiz_questions.append({
                        'question': question_text,
                        'options': options,
                        'correct': correct
                    })
                except:
                    continue

        # If pipe format failed, try alternative parsing
        if len(quiz_questions) == 0:
            patterns = re.finditer(r'(?:Q\d+|Question \d+)[:\.]?\s*(.+?)(?=(?:Q\d+|Question \d+|$))', result, re.DOTALL)

            for match in patterns:
                section = match.group(0)
                try:
                    # Extract question
                    q_match = re.search(r'(?:Q\d+|Question \d+)[:\.]?\s*(.+?)(?=[A-D][\):])', section, re.IGNORECASE)
                    if not q_match:
                        continue

                    question = q_match.group(1).strip()

                    # Skip examples
                    skip_phrases = ['what is the question', 'question text', 'example']
                    if any(phrase in question.lower() for phrase in skip_phrases):
                        continue

                    # Extract options
                    options = []
                    for letter in ['A', 'B', 'C', 'D']:
                        opt_pattern = rf'{letter}[\):\.]?\s*([^A-D\n]+?)(?=[B-D][\):]|Correct|Answer|$)'
                        opt_match = re.search(opt_pattern, section, re.IGNORECASE)
                        if opt_match:
                            opt = opt_match.group(1).strip().split('\n')[0]
                            if len(opt) < 50:
                                options.append(opt)

                    if len(options) >= 4:
                        # Find correct answer
                        correct_match = re.search(r'(?:Correct|Answer)[:\s]*([A-D])', section, re.IGNORECASE)
                        correct = correct_match.group(1).upper() if correct_match else 'A'

                        quiz_questions.append({
                            'question': question,
                            'options': options[:4],
                            'correct': correct
                        })
                except:
                    continue

        if not quiz_questions:
            return f"‚ùå Could not generate quiz for '{topic}'. The content may not have enough information about this topic."

        quiz_questions = quiz_questions[:num_questions]

        return f"‚úÖ Generated {len(quiz_questions)} questions about '{topic}'!"
    except Exception as e:
        return f"‚ùå Error: {str(e)}"

def render_quiz():
    if not quiz_questions:
        return ""

    html = '<div style="font-family: Inter, sans-serif;">'

    for i, q in enumerate(quiz_questions):
        html += f'''
        <div style="background: linear-gradient(135deg, rgba(255,255,255,0.12), rgba(255,255,255,0.08));
                    backdrop-filter: blur(15px);
                    border: 1px solid rgba(255,255,255,0.2);
                    border-radius: 16px;
                    padding: 20px;
                    margin: 15px 0;">
            <h3 style="color: white; margin-bottom: 15px;">Question {i+1}</h3>
            <p style="color: rgba(255,255,255,0.9); font-size: 1.1em; margin-bottom: 15px; font-weight: 500;">{q['question']}</p>
            <div style="margin-top: 15px;">
        '''

        for j, opt in enumerate(q['options']):
            option_letter = chr(65 + j)  # A, B, C, D
            html += f'''
                <div style="background: rgba(255,255,255,0.08);
                            padding: 12px;
                            margin: 8px 0;
                            border-radius: 8px;
                            border: 1px solid rgba(255,255,255,0.15);
                            color: white;">
                    <strong>{option_letter})</strong> {opt}
                </div>
            '''

        html += f'''
            </div>
            <p style="color: rgba(102,126,234,0.8); margin-top: 15px; font-size: 0.9em;">
                <strong>Correct Answer: {q['correct']}</strong>
            </p>
        </div>
        '''

    html += '</div>'
    return html

def export_quiz():
    if not quiz_questions:
        return None

    content = "StudyMate Quiz\n"
    content += "=" * 70 + "\n\n"

    for i, q in enumerate(quiz_questions, 1):
        content += f"Question {i}: {q['question']}\n"
        for j, opt in enumerate(q['options']):
            content += f"{chr(65+j)}) {opt}\n"
        content += f"Correct Answer: {q['correct']}\n"
        content += "-" * 70 + "\n\n"

    filename = f"studymate_quiz_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
    with open(filename, "w", encoding="utf-8") as f:
        f.write(content)

    return filename

# Improved Flashcard Generator - FIXED
def generate_flashcards_smart(topic, num_cards):
    global flashcards, current_flashcard_index

    if not pdf_texts:
        return "‚ö†Ô∏è Please upload documents first in the Data Sources tab.", render_flashcard(0)

    if not topic or not topic.strip():
        return "‚ö†Ô∏è Please enter a topic for flashcards.", render_flashcard(0)

    try:
        num_cards = int(num_cards)
        if num_cards < 1 or num_cards > 50:
            return "‚ö†Ô∏è Please enter a number between 1 and 50", render_flashcard(0)
    except:
        return "‚ö†Ô∏è Please enter a valid number", render_flashcard(0)

    flashcards = []
    current_flashcard_index = 0

    try:
        # Get topic-relevant content
        relevant_chunks = retrieve_relevant_chunks(topic, top_k=8)
        text_sample = "\n\n".join(relevant_chunks)

        if not text_sample.strip():
            return f"‚ö†Ô∏è No content found related to '{topic}'. Try a different topic.", render_flashcard(0)

        prompt = f"""Create {num_cards} flashcards about {topic} from this content.

Content:
{text_sample}

Each flashcard should have:
- FRONT: A clear, short question (5-15 words)
- BACK: A concise answer (10-30 words)

Format each as:
FRONT: question here
BACK: answer here
---

Create {num_cards} flashcards:"""

        inputs = tokenizer(prompt, return_tensors="pt", max_length=1400, truncation=True)
        if torch.cuda.is_available():
            inputs = {k: v.to('cuda') for k, v in inputs.items()}

        outputs = llm_model.generate(
            **inputs,
            max_new_tokens=1500,
            temperature=0.7,
            do_sample=True,
            top_p=0.9
        )

        result = tokenizer.decode(outputs[0], skip_special_tokens=True)

        # Parse flashcards with multiple strategies
        sections = result.split("---")

        for section in sections:
            if "FRONT:" in section and "BACK:" in section:
                try:
                    # Extract front and back
                    front_match = re.search(r'FRONT:\s*(.+?)(?=BACK:)', section, re.DOTALL | re.IGNORECASE)
                    back_match = re.search(r'BACK:\s*(.+?)(?=---|FRONT:|$)', section, re.DOTALL | re.IGNORECASE)

                    if front_match and back_match:
                        front = front_match.group(1).strip()
                        back = back_match.group(1).strip()

                        # Clean up
                        front = ' '.join(front.split())  # Remove extra whitespace
                        back = ' '.join(back.split())

                        # Validate length
                        if 10 <= len(front) <= 200 and 15 <= len(back) <= 500:
                            flashcards.append({"front": front, "back": back})
                except Exception as e:
                    print(f"Error parsing flashcard: {e}")
                    continue

        # Alternative parsing if first method fails
        if len(flashcards) == 0:
            # Try Q: and A: format
            q_a_pattern = re.finditer(r'(?:Q:|Question:)\s*(.+?)(?:A:|Answer:)\s*(.+?)(?=Q:|Question:|$)', result, re.DOTALL | re.IGNORECASE)

            for match in q_a_pattern:
                try:
                    front = match.group(1).strip()
                    back = match.group(2).strip()

                    front = ' '.join(front.split())
                    back = ' '.join(back.split())

                    if 10 <= len(front) <= 200 and 15 <= len(back) <= 500:
                        flashcards.append({"front": front, "back": back})
                except:
                    continue

        # Limit to requested number
        flashcards = flashcards[:num_cards]

        if not flashcards:
            return f"‚ùå Could not generate flashcards for '{topic}'. Try a different topic or check your documents.", render_flashcard(0)

        return f"‚úÖ Generated {len(flashcards)} flashcards on '{topic}'!", render_flashcard(0)
    except Exception as e:
        return f"‚ùå Error: {str(e)}", render_flashcard(0)

def render_flashcard(index, show_back=False):
    global current_flashcard_index

    if not flashcards:
        return """
<div style="background: linear-gradient(135deg, rgba(255,255,255,0.15), rgba(255,255,255,0.08));
            backdrop-filter: blur(15px);
            border: 1px solid rgba(255,255,255,0.3);
            border-radius: 20px;
            padding: 60px 40px;
            text-align: center;
            min-height: 300px;
            display: flex;
            align-items: center;
            justify-content: center;">
    <p style="color: rgba(255,255,255,0.7); font-size: 1.2em;">No flashcards yet. Generate some above!</p>
</div>
"""

    current_flashcard_index = index % len(flashcards)
    card = flashcards[current_flashcard_index]

    content = card["back"] if show_back else card["front"]
    label = "ANSWER" if show_back else "QUESTION"

    return f"""
<div style="background: linear-gradient(135deg, rgba(102,126,234,0.3), rgba(118,75,162,0.3));
            backdrop-filter: blur(20px);
            border: 2px solid rgba(255,255,255,0.3);
            border-radius: 20px;
            padding: 50px 40px;
            text-align: center;
            min-height: 300px;
            display: flex;
            flex-direction: column;
            justify-content: center;
            box-shadow: 0 12px 40px rgba(31,38,135,0.3);">
    <div style="color: rgba(255,255,255,0.8);
                font-size: 0.9em;
                font-weight: 600;
                margin-bottom: 20px;
                letter-spacing: 2px;">
        {label}
    </div>
    <div style="color: white;
                font-size: 1.3em;
                line-height: 1.6;
                font-weight: 500;">
        {content}
    </div>
    <div style="color: rgba(255,255,255,0.6);
                font-size: 0.85em;
                margin-top: 30px;">
        Card {current_flashcard_index + 1} of {len(flashcards)}
    </div>
</div>
"""

def next_card():
    global current_flashcard_index
    return render_flashcard(current_flashcard_index + 1, False)

def prev_card():
    global current_flashcard_index
    return render_flashcard(current_flashcard_index - 1, False)

def flip_card():
    return render_flashcard(current_flashcard_index, True)

def show_front():
    return render_flashcard(current_flashcard_index, False)

def export_flashcards():
    if not flashcards:
        return None

    content = "StudyMate Flashcards\n"
    content += "=" * 70 + "\n\n"

    for i, card in enumerate(flashcards, 1):
        content += f"Flashcard {i}\n"
        content += f"Q: {card['front']}\n"
        content += f"A: {card['back']}\n"
        content += "-" * 70 + "\n\n"

    filename = f"studymate_flashcards_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
    with open(filename, "w", encoding="utf-8") as f:
        f.write(content)

    return filename

# Audiobook with document upload - FIXED
def generate_audiobook_from_text(text_input, voice, speed):
    if not text_input or not text_input.strip():
        return "‚ö†Ô∏è Please provide text to convert", None

    try:
        lang_map = {
            "English (US) - Female": "en",
            "English (UK) - Male": "en-uk",
            "English (Australia) - Female": "en-au",
            "English (India) - Female": "en-in",
            "French": "fr",
            "Spanish": "es",
            "German": "de"
        }

        lang = lang_map.get(voice, "en")

        # Handle speed properly
        if speed == "Slow (0.75x)":
            slow = True
        else:
            slow = False

        tts = gTTS(text=text_input, lang=lang, slow=slow)

        filename = f"audiobook_{datetime.now().strftime('%Y%m%d_%H%M%S')}.mp3"
        tts.save(filename)

        char_count = len(text_input)
        word_count = len(text_input.split())

        return f"‚úÖ Audiobook generated with {voice}!\nüìä {char_count} characters ‚Ä¢ {word_count} words", filename
    except Exception as e:
        return f"‚ùå Error: {str(e)}", None

def generate_audiobook_from_doc(doc_file, voice, speed):
    if doc_file is None:
        return "‚ö†Ô∏è No document uploaded", None

    try:
        # Read the file content properly
        with open(doc_file.name, 'rb') as f:
            file_content = f.read()

        # Extract text from PDF
        text = extract_text_from_pdf(file_content)

        if text.startswith("Error"):
            return f"‚ùå {text}", None

        # Limit text length for reasonable audio file size
        text = text[:5000]  # First 5000 characters

        return generate_audiobook_from_text(text, voice, speed)
    except Exception as e:
        return f"‚ùå Error: {str(e)}", None

def export_history():
    if not qa_history:
        return None

    content = "StudyMate Q&A History\n"
    content += "=" * 70 + "\n\n"

    for i, qa in enumerate(qa_history, 1):
        content += f"Question {i}: {qa['question']}\n"
        content += f"Answer {i}: {qa['answer']}\n"
        content += f"Timestamp: {qa['timestamp']}\n"
        content += "-" * 70 + "\n\n"

    filename = f"studymate_history_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
    with open(filename, "w", encoding="utf-8") as f:
        f.write(content)

    return filename

def get_stats():
    stats = f"""
üìä **Session Statistics**

‚Ä¢ Documents Loaded: **{len(pdf_names)}**
‚Ä¢ Questions Asked: **{len(qa_history)}**
‚Ä¢ Flashcards Created: **{len(flashcards)}**
    """
    return stats

# Custom CSS
custom_css = """
@import url('https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap');

* {
    font-family: 'Inter', sans-serif !important;
}

body {
    background: linear-gradient(135deg, #667eea 0%, #764ba2 50%, #f093fb 100%) !important;
    background-attachment: fixed !important;
    background-size: 400% 400% !important;
    animation: gradientShift 15s ease infinite !important;
}

@keyframes gradientShift {
    0% { background-position: 0% 50%; }
    50% { background-position: 100% 50%; }
    100% { background-position: 0% 50%; }
}

.gradio-container {
    max-width: 1400px !important;
    margin: 0 auto !important;
    padding: 24px !important;
    background: rgba(255, 255, 255, 0.05) !important;
    backdrop-filter: blur(20px) !important;
    border: 1px solid rgba(255, 255, 255, 0.18) !important;
    border-radius: 24px !important;
    box-shadow: 0 8px 32px 0 rgba(31, 38, 135, 0.37) !important;
}

#header {
    background: linear-gradient(135deg, rgba(255, 255, 255, 0.1), rgba(255, 255, 255, 0.05)) !important;
    backdrop-filter: blur(15px) !important;
    border: 1px solid rgba(255, 255, 255, 0.2) !important;
    border-radius: 20px !important;
    padding: 32px !important;
    margin-bottom: 24px !important;
    box-shadow: 0 8px 32px 0 rgba(31, 38, 135, 0.2) !important;
    text-align: center !important;
}

h1 {
    background: linear-gradient(135deg, #ffffff 0%, #e0c3fc 100%) !important;
    -webkit-background-clip: text !important;
    -webkit-text-fill-color: transparent !important;
    font-size: 2.5em !important;
    font-weight: 700 !important;
    margin: 0 0 8px 0 !important;
}

.subtitle {
    color: rgba(255, 255, 255, 0.8) !important;
    font-size: 1.1em !important;
    font-weight: 400 !important;
}

button.svelte-1b6s6s {
    background: rgba(255, 255, 255, 0.08) !important;
    backdrop-filter: blur(8px) !important;
    border: 1px solid rgba(255, 255, 255, 0.15) !important;
    border-radius: 12px !important;
    color: rgba(255, 255, 255, 0.85) !important;
    font-weight: 500 !important;
    padding: 12px 20px !important;
    transition: all 0.3s ease !important;
}

button.svelte-1b6s6s:hover {
    background: rgba(255, 255, 255, 0.12) !important;
}

button.selected {
    background: linear-gradient(135deg, rgba(102, 126, 234, 0.5), rgba(118, 75, 162, 0.5)) !important;
    color: white !important;
}

.gr-group, .gr-box {
    background: linear-gradient(135deg, rgba(255, 255, 255, 0.12), rgba(255, 255, 255, 0.08)) !important;
    backdrop-filter: blur(15px) !important;
    border: 1px solid rgba(255, 255, 255, 0.2) !important;
    border-radius: 16px !important;
    padding: 20px !important;
    margin: 12px 0 !important;
    box-shadow: 0 8px 32px 0 rgba(31, 38, 135, 0.15) !important;
}

.gr-group h3, .gr-group h4 {
    color: rgba(255, 255, 255, 0.95) !important;
    font-weight: 600 !important;
    margin: 0 0 12px 0 !important;
    padding-bottom: 8px !important;
    border-bottom: 1px solid rgba(255, 255, 255, 0.1) !important;
}

.gr-button {
    background: linear-gradient(135deg, rgba(255, 255, 255, 0.18), rgba(255, 255, 255, 0.12)) !important;
    backdrop-filter: blur(10px) !important;
    border: 1px solid rgba(255, 255, 255, 0.25) !important;
    border-radius: 10px !important;
    color: white !important;
    font-weight: 600 !important;
    padding: 10px 20px !important;
    transition: all 0.3s ease !important;
}

.gr-button:hover {
    background: linear-gradient(135deg, rgba(255, 255, 255, 0.25), rgba(255, 255, 255, 0.18)) !important;
    transform: translateY(-2px) !important;
}

.gr-button-primary {
    background: linear-gradient(135deg, rgba(102, 126, 234, 0.6), rgba(118, 75, 162, 0.6)) !important;
}

.gr-textbox, textarea, input {
    background: rgba(255, 255, 255, 0.1) !important;
    backdrop-filter: blur(10px) !important;
    border: 1px solid rgba(255, 255, 255, 0.2) !important;
    border-radius: 10px !important;
    color: white !important;
    padding: 12px !important;
}

.gr-textbox:focus, textarea:focus, input:focus {
    background: rgba(255, 255, 255, 0.15) !important;
    border: 1px solid rgba(255, 255, 255, 0.35) !important;
    outline: none !important;
}

#chatbot {
    background: rgba(255, 255, 255, 0.05) !important;
    backdrop-filter: blur(15px) !important;
    border: 1px solid rgba(255, 255, 255, 0.15) !important;
    border-radius: 16px !important;
    padding: 16px !important;
}

.gr-file {
    background: rgba(255, 255, 255, 0.08) !important;
    backdrop-filter: blur(10px) !important;
    border: 2px dashed rgba(255, 255, 255, 0.25) !important;
    border-radius: 12px !important;
    padding: 20px !important;
}

label {
    color: rgba(255, 255, 255, 0.85) !important;
    font-weight: 500 !important;
}

.stats-box {
    background: linear-gradient(135deg, rgba(102, 126, 234, 0.2), rgba(118, 75, 162, 0.2)) !important;
    backdrop-filter: blur(10px) !important;
    border: 1px solid rgba(255, 255, 255, 0.25) !important;
    border-radius: 12px !important;
    padding: 16px !important;
}

::-webkit-scrollbar {
    width: 8px !important;
}

::-webkit-scrollbar-thumb {
    background: linear-gradient(135deg, rgba(102, 126, 234, 0.6), rgba(118, 75, 162, 0.6)) !important;
    border-radius: 10px !important;
}

::placeholder {
    color: rgba(255, 255, 255, 0.4) !important;
}
"""

# Initialize models
initialize_models()

# Create Gradio Interface
with gr.Blocks(css=custom_css, theme=gr.themes.Soft(), title="StudyMate") as app:

    gr.Markdown(
        """
        # üéì StudyMate
        <p class="subtitle">Your AI-Powered Study Companion</p>
        """,
        elem_id="header"
    )

    stats_display = gr.Markdown(get_stats(), elem_classes="stats-box")

    with gr.Tabs():

        # DATA SOURCES TAB
        with gr.Tab("üìÅ Data Sources"):
            gr.Markdown("### Upload your study materials")

            with gr.Row():
                with gr.Column():
                    with gr.Group():
                        gr.Markdown("#### üìÑ PDF Documents")
                        pdf_input = gr.File(label="Upload PDFs", file_count="multiple", file_types=[".pdf"])
                        upload_btn = gr.Button("Process PDFs", variant="primary", size="lg")
                        upload_status = gr.Textbox(label="Status", lines=2, interactive=False)

                with gr.Column():
                    with gr.Group():
                        gr.Markdown("#### üì∏ Images (OCR)")
                        image_input = gr.Image(label="Upload image", type="pil")
                        ocr_btn = gr.Button("Extract Text", variant="primary", size="lg")
                        ocr_output = gr.Textbox(label="Result", lines=4, interactive=False)

            with gr.Group():
                gr.Markdown("#### üìö Loaded Documents")
                pdf_list = gr.Textbox(label="Documents", lines=6, interactive=False)

        # CONVERSATIONAL AI TAB
        with gr.Tab("üí¨ Conversational AI"):
            gr.Markdown("### Ask questions about your documents")

            with gr.Row():
                with gr.Column(scale=2):
                    chatbot = gr.Chatbot(label="Conversation", height=450, bubble_full_width=False)
                    question_input = gr.Textbox(label="Your question", placeholder="Type here...", lines=2)
                    with gr.Row():
                        ask_btn = gr.Button("Ask", variant="primary", scale=2)
                        clear_btn = gr.Button("Clear", scale=1)
                    export_btn = gr.Button("Download History", variant="secondary")
                    export_file = gr.File(label="Download", interactive=False)

                with gr.Column(scale=1):
                    with gr.Group():
                        gr.Markdown("#### üîç Sources")
                        sources_display = gr.Textbox(label="References", lines=20, interactive=False)

        # VOICE ASSISTANT TAB
        with gr.Tab("üé§ Voice Assistant"):
            gr.Markdown("### Speak your questions and hear the answers")

            with gr.Row():
                with gr.Column():
                    with gr.Group():
                        gr.Markdown("#### üéôÔ∏è Record Your Question")
                        audio_input = gr.Audio(sources=["microphone"], type="filepath", label="Click to record")
                        voice_btn = gr.Button("üéØ Process Voice", variant="primary", size="lg")
                        voice_output = gr.Textbox(label="Transcription", lines=2, interactive=False)

                    with gr.Group():
                        gr.Markdown("#### üîä Audio Response")
                        voice_audio_output = gr.Audio(label="Listen to Answer", interactive=False, autoplay=True)

                    voice_chatbot = gr.Chatbot(label="Conversation History", height=400)

                with gr.Column():
                    with gr.Group():
                        gr.Markdown("#### üí° How to Use")
                        gr.Markdown("""
                        1. **Upload documents** first in Data Sources tab
                        2. Click the **microphone icon** to start recording
                        3. **Speak your question** clearly
                        4. Click **'Process Voice'**
                        5. **Read the text answer** in conversation
                        6. **Listen to the audio answer** that plays automatically

                        **Tip:** The answer will play automatically after processing!
                        """)

        # QUIZME TAB
        with gr.Tab("üìù QuizMe"):
            gr.Markdown("### Generate topic-based quizzes")

            with gr.Group():
                gr.Markdown("#### ‚öôÔ∏è Quiz Settings")
                quiz_topic = gr.Textbox(label="Topic", placeholder="e.g., 'photosynthesis', 'world war 2', etc.")
                quiz_count = gr.Number(label="Number of questions", value=5, minimum=1, maximum=10)

                with gr.Row():
                    quiz_gen_btn = gr.Button("üéØ Generate Quiz", variant="primary", size="lg")
                    export_quiz_btn = gr.Button("üì• Download Quiz", variant="secondary", size="lg")

                quiz_status = gr.Textbox(label="Status", lines=2, interactive=False)
                quiz_file = gr.File(label="Download", interactive=False)

        # FLASHCARDS TAB
        with gr.Tab("üÉè Flashcards"):
            gr.Markdown("### Interactive flashcard study tool")

            with gr.Row():
                with gr.Column(scale=1):
                    with gr.Group():
                        gr.Markdown("#### ‚öôÔ∏è Settings")
                        flashcard_topic = gr.Textbox(label="Topic", placeholder="e.g., 'biology terms'")
                        flashcard_count = gr.Number(label="Number of cards", value=10, minimum=1, maximum=50)
                        flashcard_gen_btn = gr.Button("Generate", variant="primary", size="lg")
                        flashcard_status = gr.Textbox(label="Status", lines=2, interactive=False)

                    with gr.Group():
                        flashcard_export_btn = gr.Button("Download Flashcards", variant="secondary")
                        flashcard_file = gr.File(label="Download", interactive=False)

                with gr.Column(scale=2):
                    with gr.Group():
                        gr.Markdown("#### üé¥ Flashcard Viewer")
                        flashcard_display = gr.HTML(render_flashcard(0))
                        with gr.Row():
                            prev_btn = gr.Button("‚Üê Previous", scale=1)
                            flip_btn = gr.Button("üîÑ Flip", variant="primary", scale=2)
                            next_btn = gr.Button("Next ‚Üí", scale=1)
                        show_front_btn = gr.Button("Show Question", variant="secondary")

        # AUDIOBOOK TAB
        with gr.Tab("üéß Audiobook"):
            gr.Markdown("### Convert text or documents to audio")

            with gr.Row():
                with gr.Column():
                    with gr.Group():
                        gr.Markdown("#### üìù Text Input")
                        audiobook_text = gr.Textbox(label="Paste text", lines=10, placeholder="Enter text...")

                    with gr.Group():
                        gr.Markdown("#### üìÑ Or Upload Document")
                        audiobook_doc = gr.File(label="Upload PDF", file_types=[".pdf"])

                with gr.Column():
                    with gr.Group():
                        gr.Markdown("#### ‚öôÔ∏è Voice Settings")
                        voice_select = gr.Dropdown(
                            label="Voice",
                            choices=[
                                "English (US) - Female",
                                "English (UK) - Male",
                                "English (Australia) - Female",
                                "English (India) - Female",
                                "French",
                                "Spanish",
                                "German"
                            ],
                            value="English (US) - Female"
                        )
                        speed_select = gr.Dropdown(
                            label="Speed",
                            choices=["Slow (0.75x)", "Normal (1x)", "Fast (1.25x)"],
                            value="Normal (1x)"
                        )

                        with gr.Row():
                            gen_text_btn = gr.Button("Generate from Text", variant="primary")
                            gen_doc_btn = gr.Button("Generate from Document", variant="primary")

                        audiobook_status = gr.Textbox(label="Status", lines=2, interactive=False)
                        audiobook_output = gr.Audio(label="Audio", interactive=False)

    gr.Markdown("""
        ---
        <div style="text-align: center; color: rgba(255,255,255,0.6); padding: 16px;">
            <p>Powered by IBM Granite 3.3 2B ‚Ä¢ Built with Gradio</p>
        </div>
    """)

    # Event Handlers
    upload_btn.click(process_pdfs, [pdf_input], [upload_status, pdf_list, stats_display])
    ocr_btn.click(process_image_ocr, [image_input], [ocr_output, stats_display])

    ask_btn.click(answer_question, [question_input, chatbot], [chatbot, question_input, sources_display, stats_display])
    question_input.submit(answer_question, [question_input, chatbot], [chatbot, question_input, sources_display, stats_display])
    clear_btn.click(lambda: ([], ""), outputs=[chatbot, sources_display])
    export_btn.click(export_history, outputs=[export_file])

    voice_btn.click(process_voice_input, [audio_input, voice_chatbot], [voice_output, voice_chatbot, voice_audio_output])

    quiz_gen_btn.click(generate_quiz, [quiz_topic, quiz_count], [quiz_status])

    export_quiz_btn.click(export_quiz, outputs=[quiz_file])

    flashcard_gen_btn.click(generate_flashcards_smart, [flashcard_topic, flashcard_count], [flashcard_status, flashcard_display])
    next_btn.click(next_card, outputs=[flashcard_display])
    prev_btn.click(prev_card, outputs=[flashcard_display])
    flip_btn.click(flip_card, outputs=[flashcard_display])
    show_front_btn.click(show_front, outputs=[flashcard_display])
    flashcard_export_btn.click(export_flashcards, outputs=[flashcard_file])

    gen_text_btn.click(generate_audiobook_from_text, [audiobook_text, voice_select, speed_select], [audiobook_status, audiobook_output])
    gen_doc_btn.click(generate_audiobook_from_doc, [audiobook_doc, voice_select, speed_select], [audiobook_status, audiobook_output])

app.launch(share=True, debug=True)